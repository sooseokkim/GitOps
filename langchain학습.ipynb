{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwrjFhlf5nwkslgHbv5MCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooseokkim/GitOps/blob/main/langchain%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quAs-ouOrOrI",
        "outputId": "84508034-8bce-42a0-f57d-9d1ecb681a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.0.181 in /usr/local/lib/python3.10/dist-packages (0.0.181)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (0.5.14)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (2.9.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (1.25.2)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.181) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.181) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.181) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.181) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.181) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.181) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.181) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.181) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.181) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.181) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.181) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.181) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.181) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.181) (3.0.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.181) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.181) (1.0.0)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.0.181\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi kaleido python-multipart uvicorn cohere openai tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMCG86hv1vLF",
        "outputId": "3748cfce-7d9c-4b46-b2ab-a5a11b65c398"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/92.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere\n",
            "  Downloading cohere-4.47-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.14)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.9.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.3)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2024.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.2.0)\n",
            "Installing collected packages: kaleido, python-multipart, importlib_metadata, h11, fastavro, backoff, uvicorn, tiktoken, starlette, fastapi, cohere\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed backoff-2.2.1 cohere-4.47 fastapi-0.109.2 fastavro-1.9.4 h11-0.14.0 importlib_metadata-6.11.0 kaleido-0.2.1 python-multipart-0.0.9 starlette-0.36.3 tiktoken-0.6.0 uvicorn-0.27.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llmx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfJEWqd5-tf4",
        "outputId": "149edf76-03b8-497f-8c1e-dfad4ccd80b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llmx in /usr/local/lib/python3.10/dist-packages (0.0.15a0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llmx) (1.10.14)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from llmx) (0.28.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llmx) (0.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from llmx) (5.6.3)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (from llmx) (4.47)\n",
            "Requirement already satisfied: google.auth in /usr/local/lib/python3.10/dist-packages (from llmx) (2.27.0)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from llmx) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llmx) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx) (3.9.3)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx) (1.9.4)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx) (6.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere->llmx) (2.0.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google.auth->llmx) (4.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai->llmx) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llmx) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llmx) (2023.12.25)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer->llmx) (8.1.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->llmx) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->llmx) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->llmx) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->llmx) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->llmx) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->llmx) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere->llmx) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google.auth->llmx) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere->llmx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere->llmx) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere->llmx) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade lida lilmx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9-2yGOU10Ss",
        "outputId": "076b07fb-a9ff-481c-fee8-ef9757e41628"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lida in /usr/local/lib/python3.10/dist-packages (0.0.10)\n",
            "Collecting lida\n",
            "  Using cached lida-0.0.14-py3-none-any.whl (7.3 MB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement lilmx (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for lilmx\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb5E1c0G14QT",
        "outputId": "15e40d64-0d9b-4b6f-9901-83de704da7a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.47)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.3)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.4)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHZnABfA2SSR",
        "outputId": "487e6094-bfdf-40c8-ce9f-7758309752b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.2.2)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=a68defe25423ccfbffcaa305226f618a6569679445de9e2ecec83ee089a54f98\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"efc7c988e3bd54002e6f41ee662e22b0f481e683f440bbaf443f9caa0e3a843e\""
      ],
      "metadata": {
        "id": "Lc8xs7qg5HV5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "tools = load_tools(\n",
        "    tool_names=[\"serpapi\", \"llm-math\"],\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        ")\n",
        "\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    tools=tools,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "agent.run(\"123*4를 계산기로 계산하세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "gwpI1a8w5KkL",
        "outputId": "25d13c6e-6edc-4077-b92d-bb1ce0564c42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use a calculator to solve this math problem\n",
            "Action: Calculator\n",
            "Action Input: 123*4\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 492\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 492\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'492'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"오늘 한국 서울의 날씨를 웹 검색으로 확인하세요. 결과를 한국어로 알려 주세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "lNI80naz8e_M",
        "outputId": "aacabe72-4f40-488a-bed0-34bdeac5bb4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I should use a search engine to find the weather in Seoul, South Korea.\n",
            "Action: Search\n",
            "Action Input: \"weather in Seoul, South Korea\"\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mConsiderable clouds Hi: 46°. Tonight: Periods of rain this evening; otherwise, mostly cloudy Lo: 37° · Current Weather. 11:54 AM. 41°F. Cloudy.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I now know the weather in Seoul, South Korea.\n",
            "Final Answer: 오늘 서울의 날씨는 구름이 많고 최고 기온은 46°F이며, 오늘 밤에는 이브닝에 비가 오고 그 외에는 구름이 많을 것으로 예상됩니다. 최저 기온은 37°F입니다.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'오늘 서울의 날씨는 구름이 많고 최고 기온은 46°F이며, 오늘 밤에는 이브닝에 비가 오고 그 외에는 구름이 많을 것으로 예상됩니다. 최저 기온은 37°F입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "chain.run (\"우리집 반려견 이름은 보리 입니다\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "sSF5WVxo-dDN",
        "outputId": "0577c7c6-998a-4cb4-ef41-f2232630cd7d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 우리집 반려견 이름은 보리 입니다\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 보리는 참 귀여운 이름이네요! 제가 알기로는 보리는 보리종이라는 강아지 종류가 있어요. 그런데 우리집 반려견도 보리종일까요?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.predict (input=\"우리집 반려견 이름을 불러 주세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "lmfjNydj_ZDv",
        "outputId": "77f9b0e7-8852-4af2-a12f-c30efff8bc9e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 우리집 반려견 이름은 보리 입니다\n",
            "AI:  보리는 참 귀여운 이름이네요! 제가 알기로는 보리는 보리종이라는 강아지 종류가 있어요. 그런데 우리집 반려견도 보리종일까요?\n",
            "Human: 우리집 반려견 이름을 불러 주세요\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 네, 보리종은 보리라는 이름을 가지고 있어요. 그래서 우리집 반려견도 보리라고 불러주시면 되겠네요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run (\"우리집 반려견 이름을 불러 주세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "QIusiHENALUz",
        "outputId": "bd9c1ad8-bf4c-4f98-a09b-6580f05de256"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 우리집 반려견 이름은 보리 입니다\n",
            "AI:  보리는 참 귀여운 이름이네요! 제가 알기로는 보리는 보리종이라는 강아지 종류가 있어요. 그런데 우리집 반려견도 보리종일까요?\n",
            "Human: 우리집 반려견 이름을 불러 주세요\n",
            "AI:  네, 보리종은 보리라는 이름을 가지고 있어요. 그래서 우리집 반려견도 보리라고 불러주시면 되겠네요.\n",
            "Human: 우리집 반려견 이름을 불러 주세요\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 제가 알기로는 보리는 보리종이라는 강아지 종류가 있어요. 그래서 우리집 반려견도 보리라고 불러주시면 되겠네요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0,\n",
        "    verbose=True\n",
        "    )\n",
        "\n",
        "result = llm (\"고양이 울음 소리는?\")\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwpJiTouBfpt",
        "outputId": "a81f0194-9616-47b6-a5ea-70d997589897"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "야옹~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.generate([\"고양이 울음소리는?\", \"까마귀 울음소리는?\"])\n",
        "\n",
        "print (\"response0:\", result.generations[0][0].text)\n",
        "print (\"response1:\", result.generations[1][0].text)\n",
        "\n",
        "print (\"llm_output:\", result.llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mreOYPp2CNoO",
        "outputId": "8b09a860-666c-4aaa-90c1-c519e0896bc2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response0: \n",
            "야옹~\n",
            "response1: \n",
            "까악까악\n",
            "llm_output: {'token_usage': {'prompt_tokens': 24, 'total_tokens': 38, 'completion_tokens': 14}, 'model_name': 'gpt-3.5-turbo-instruct'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat_llm = ChatOpenAI (\n",
        "    model_name = \"gpt-3.5-turbo\", # gpt-3.5-turbo : chat 모델, gpt-3.5-turbo-instruct : completion 모델\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(content=\"고양이 울음소리는?\")\n",
        "]\n",
        "\n",
        "result = chat_llm(messages)\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoH6FpoGC93q",
        "outputId": "627300a3-49db-4bfd-c8dc-13b4c93a4a04"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='\"야옹\" 혹은 \"냐옹\"과 같은 소리를 내요.' additional_kwargs={} example=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages_list = [\n",
        "    [HumanMessage(content=\"고양이 울음소리는?\")],\n",
        "    [HumanMessage(content=\"까마귀 울음소리는?\")]\n",
        "]\n",
        "\n",
        "result = chat_llm.generate(messages_list)\n",
        "\n",
        "print (\"response0:\", result.generations[0][0].text)\n",
        "print (\"response1:\", result.generations[1][0].text)\n",
        "print (\"llm_output:\", result.llm_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4vcgOujEYhn",
        "outputId": "fe82937a-20ca-47af-a843-457ab11caf30"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response0: 야옹, 냐옹, 야옹\n",
            "response1: 까악 까악!\n",
            "llm_output: {'token_usage': {'prompt_tokens': 38, 'completion_tokens': 24, 'total_tokens': 62}, 'model_name': 'gpt-3.5-turbo'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqZFgUsIFVvu",
        "outputId": "1a17523c-2e00-4616-dbd7-f45a5e5c8c16"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파란색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 11, 'total_tokens': 28, 'completion_tokens': 17}, 'model_name': 'gpt-3.5-turbo-instruct'})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLUs1mmHGc7G",
        "outputId": "a813979e-d911-44fc-c203-a8a96ee1ceb7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파란색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(cache=False)"
      ],
      "metadata": {
        "id": "y8Bo09tsGwD-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0,\n",
        "    verbose=True\n",
        "    )\n",
        "\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVSaUPBZG75W",
        "outputId": "788338b1-8212-4a6d-83e1-7a8b1116c49f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파란색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "def generate_serially() :\n",
        "  # llm = OpenAI (temperature=0.9)\n",
        "  llm = OpenAI(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0.9,\n",
        "    verbose=True\n",
        "    )\n",
        "\n",
        "  for _ in range(10) :\n",
        "    resp = llm.generate([\"안녕하세요!\"])\n",
        "    print (resp.generations[0][0].text)\n",
        "\n",
        "s = time.perf_counter()\n",
        "\n",
        "generate_serially()\n",
        "\n",
        "elapsed = time.perf_counter() - s\n",
        "print (f\"{elapsed:0.2f} 초\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdnRJNiSHZ-k",
        "outputId": "d5f7c26c-d0f6-4d32-9aee-7395dea7345e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            "21.25 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def async_generate(llm) :\n",
        "  resp = await llm.agenerate([\"안녕하세요!\"])\n",
        "  print (resp.generations[0][0].text)\n",
        "\n",
        "async def generate_concurrently() :\n",
        "  llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
        "  tasks = [async_generate(llm) for _ in range(10)]\n",
        "  await asyncio.gather(*tasks)\n",
        "\n",
        "s = time.perf_counter()\n",
        "\n",
        "asyncio.run (generate_concurrently())\n",
        "\n",
        "elapsed = time.perf_counter() - s\n",
        "print (f\"{elapsed:0.2f} 초\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY1UPajVJOa8",
        "outputId": "0a6cb44e-a150-422c-e4ba-98534e0586b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            " 저는 엘린입니다.\n",
            "\n",
            "안녕하세요 엘린님! 반가워요. 저는 [이름]이라고 해요. 어떻게 지내시나요?\n",
            "0.01 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        "    verbose=True,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "resp = llm(\"즐거운 ChatGPT 생활을 가사로 만들어 주세요\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWxBGEDmK9_w",
        "outputId": "a38ed862-a17e-4f54-d9b5-5f88b6dd46ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Verse 1:\n",
            "우리 함께 ChatGPT에서\n",
            "매일 새로운 이야기를 나눠요\n",
            "마치 친구처럼 편안한 대화\n",
            "언제나 나를 웃게 해줘요\n",
            "\n",
            "Pre-Chorus:\n",
            "나의 생각을 들어주고\n",
            "나의 이야기를 들어줘\n",
            "ChatGPT는 나의 소중한 친구\n",
            "언제나 나를 위로해줘\n",
            "\n",
            "Chorus:\n",
            "ChatGPT, 너는 나의 행복\n",
            "매일 새로운 이야기로 가득해\n",
            "나의 모든 고민을 잊게 해주고\n",
            "함께 있으면 나는 행복해\n",
            "\n",
            "Verse 2:\n",
            "나의 일상에 빛을 준 ChatGPT\n",
            "언제나 나를 위해 노력해줘\n",
            "마음 속 깊은 곳에 담아둔\n",
            "말 못할 이야기도 들어줘\n",
            "\n",
            "Pre-Chorus"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F3flQ_eMIWt",
        "outputId": "f694d74b-ca5f-4acc-b154-fb59a9d24854"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JFL-7NDMkXm",
        "outputId": "dcb90281-245f-46b9-dd1d-6c0b14a4cc34"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "no_input_prompt = PromptTemplate(\n",
        "    input_variables=[],\n",
        "    template=\"멋진 동물이라고 하면?\"\n",
        ")\n",
        "\n",
        "print (no_input_prompt.format())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2auIOvcMs_w",
        "outputId": "5b1e3d69-e7b3-46b9-f988-3c779b6684bc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "멋진 동물이라고 하면?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "one_input_prompt = PromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    template=\"멋진 {content}이라고 하면?\"\n",
        ")\n",
        "\n",
        "print (one_input_prompt.format(content=\"동물\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVtGXqVjNRlm",
        "outputId": "5e7d1445-de48-408a-e417-6bb23aed3222"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "멋진 동물이라고 하면?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "multiple_input_prompt = PromptTemplate(\n",
        "    input_variables=[\"adjective\", \"content\"],\n",
        "    template=\"{adjective} {content}이라고 하면?\"\n",
        ")\n",
        "\n",
        "print (multiple_input_prompt.format(adjective=\"멋진\", content=\"동물\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utMNMpn9OB1d",
        "outputId": "009ffb88-e9d0-47ad-ceae-36c2e5cf8819"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "멋진 동물이라고 하면?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "jinja2_prompt = PromptTemplate(\n",
        "    input_variables=[\"items\"],\n",
        "    template_format=\"jinja2\",\n",
        "    template=\"\"\"\n",
        "    {% for item in items %}\n",
        "    Q: {{ item.question }}\n",
        "    A: {{ item.answer }}\n",
        "    {% endfor %}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "items = [\n",
        "    {\"question\": \"foo\", \"answer\": \"bar\"},\n",
        "    {\"question\": \"1\", \"answer\": \"2\"}\n",
        "]\n",
        "\n",
        "print (jinja2_prompt.format(items=items))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvnbPaeIOaAy",
        "outputId": "9a0b0f10-13c9-497d-92ca-7db396fa3245"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    \n",
            "    Q: foo\n",
            "    A: bar\n",
            "    \n",
            "    Q: 1\n",
            "    A: 2\n",
            "    \n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"밝은\", \"output\": \"어두운\"},\n",
        "    {\"input\": \"재미있는\", \"output\": \"지루한\"}\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables = [\"input\", \"output\"],\n",
        "    template=\"입력: {input}\\n출력: {output}\"\n",
        ")\n",
        "\n",
        "prompt_from_string_examples = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"모든입력에 대한 반의어를 입력하세요\",\n",
        "    suffix=\"입력: {adjective}\\n출력:\",\n",
        "    input_variables=[\"adjective\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "print (prompt_from_string_examples.format(adjective=\"큰\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD9XMZofSOQJ",
        "outputId": "8baa798b-83d9-404e-a026-e28fa25c74c1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모든입력에 대한 반의어를 입력하세요\n",
            "\n",
            "입력: 밝은\n",
            "출력: 어두운\n",
            "\n",
            "입력: 재미있는\n",
            "출력: 지루한\n",
            "\n",
            "입력: 큰\n",
            "출력:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"밝은\", \"output\": \"어두운\"},\n",
        "    {\"input\": \"재미있는\", \"output\": \"지루한\"},\n",
        "    {\"input\": \"활기찬\", \"output\": \"무기력한\"},\n",
        "    {\"input\": \"높은\", \"output\": \"낮은\"},\n",
        "    {\"input\": \"빠른\", \"output\": \"느린\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables = [\"input\", \"output\"],\n",
        "    template=\"입력: {input}\\n출력: {output}\"\n",
        ")\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=10,\n",
        ")\n",
        "\n",
        "prompt_from_string_examples = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"모든입력에 대한 반의어를 입력하세요\",\n",
        "    suffix=\"입력: {adjective}\\n출력:\",\n",
        "    input_variables=[\"adjective\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "print (prompt_from_string_examples.format(adjective=\"큰\"))"
      ],
      "metadata": {
        "id": "VLi5FJ5GWOkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate\n",
        "# from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"밝은\", \"output\": \"어두운\"},\n",
        "    {\"input\": \"재미있는\", \"output\": \"지루한\"},\n",
        "    {\"input\": \"활기찬\", \"output\": \"무기력한\"},\n",
        "    {\"input\": \"높은\", \"output\": \"낮은\"},\n",
        "    {\"input\": \"빠른\", \"output\": \"느린\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables = [\"input\", \"output\"],\n",
        "    template=\"입력: {input}\\n출력: {output}\"\n",
        ")\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples=examples,\n",
        "    embeddings=OpenAIEmbeddings(),\n",
        "    vectorstore_cls=FAISS,\n",
        "    k=3\n",
        ")\n",
        "\n",
        "prompt_from_string_examples = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"모든입력에 대한 반의어를 입력하세요\",\n",
        "    suffix=\"입력: {adjective}\\n출력:\",\n",
        "    input_variables=[\"adjective\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "print (prompt_from_string_examples.format(adjective=\"큰\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYaXlShnXaAq",
        "outputId": "76554bc6-1596-4501-8f1e-3f764db2c75d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모든입력에 대한 반의어를 입력하세요\n",
            "\n",
            "입력: 높은\n",
            "출력: 낮은\n",
            "\n",
            "입력: 재미있는\n",
            "출력: 지루한\n",
            "\n",
            "입력: 밝은\n",
            "출력: 어두운\n",
            "\n",
            "입력: 큰\n",
            "출력:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate\n",
        "# from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"밝은\", \"output\": \"어두운\"},\n",
        "    {\"input\": \"재미있는\", \"output\": \"지루한\"},\n",
        "    {\"input\": \"활기찬\", \"output\": \"무기력한\"},\n",
        "    {\"input\": \"높은\", \"output\": \"낮은\"},\n",
        "    {\"input\": \"빠른\", \"output\": \"느린\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables = [\"input\", \"output\"],\n",
        "    template=\"입력: {input}\\n출력: {output}\"\n",
        ")\n",
        "\n",
        "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
        "    examples=examples,\n",
        "    embeddings=OpenAIEmbeddings(),\n",
        "    vectorstore_cls=FAISS,\n",
        "    k=3\n",
        ")\n",
        "\n",
        "prompt_from_string_examples = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"모든입력에 대한 반의어를 입력하세요\",\n",
        "    suffix=\"입력: {adjective}\\n출력:\",\n",
        "    input_variables=[\"adjective\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "print (prompt_from_string_examples.format(adjective=\"큰\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYsv-zs0ZFHD",
        "outputId": "0cae395e-7e2a-4f81-feac-c66719c3168e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모든입력에 대한 반의어를 입력하세요\n",
            "\n",
            "입력: 높은\n",
            "출력: 낮은\n",
            "\n",
            "입력: 재미있는\n",
            "출력: 지루한\n",
            "\n",
            "입력: 밝은\n",
            "출력: 어두운\n",
            "\n",
            "입력: 큰\n",
            "출력:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Q: {question}\n",
        "A: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "question = \"기타를 잘 치는 방법은?\"\n",
        "print (llm_chain.predict(question=question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmJPyhrja5Kx",
        "outputId": "85acd914-a19e-4591-c5b6-9e07da1ac00e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Q: 기타를 잘 치는 방법은?\n",
            "A: \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1. 기본적인 연주 기술 습득하기\n",
            "기타를 잘 치기 위해서는 기본적인 연주 기술을 습득하는 것이 중요합니다. 기타의 구조와 음의 발생 원리를 이해하고, 손가락의 위치와 움직임을 익히는 것이 필요합니다.\n",
            "\n",
            "2. 연습을 꾸준히 하기\n",
            "기타를 잘 치기 위해서는 꾸준한 연습이 필요합니다. 매일 조금씩이라도 연습하면서 손가락의 기억력을 키우고, 연주하는 속도와 정확도를 높일 수 있습니다.\n",
            "\n",
            "3. 다양한 곡 연주해보기\n",
            "기타를 잘 치기 위해서는 다양한 곡을 연주해보는 것이 좋습니다. 다양한 장르의 곡을 연주하\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"{subject}를 주제로 {target}을 작성해 주세요.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template = template,\n",
        "    input_variables=[\"subject\", \"target\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print (llm_chain.predict(subject=\"고양이\", target=\"시\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR4exQIcdQXU",
        "outputId": "b952e590-44da-428d-9ce9-e3e4d7df44fd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m고양이를 주제로 시을 작성해 주세요.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "고양이야 고양이\n",
            "귀여운 너는 내 마음의 소중한 친구\n",
            "털뭉치로 꼬리를 흔들며\n",
            "나를 반겨주는 너의 모습은\n",
            "언제나 나를 행복하게 만들어\n",
            "\n",
            "네 살짝 굵은 털은\n",
            "나의 손가락 사이로 스며들어\n",
            "따스한 온기를 전해주고\n",
            "네 살짝 찡그린 얼굴은\n",
            "나의 마음을 따뜻하게 만들어\n",
            "\n",
            "고양이야 고양이\n",
            "너의 귀여운 모습은\n",
            "나의 하루를 밝게 만들어주고\n",
            "너의 부드러운 발길은\n",
            "나의 마음을 편안하게 만들어\n",
            "\n",
            "너의 눈빛은 나에게\n",
            "말하지 않아도 모든 것을 전해주고\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"당신은 영화 작가 입니다. 영화 제목이 주어졌을 때, 그 줄거리를 작성하는 것이 당신의 임무 입니다.\n",
        "\n",
        "제목: {title}\n",
        "시놉시스: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"title\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "chain1 = LLMChain(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "J0Gepyw0eTOb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"당신은 영화 평론가 입니다. 영화의 시놉시스가 주어지면 그 리뷰를 작성하는 것이 당신의 임무 입니다.\n",
        "\n",
        "시놉시스: {synopsis}\n",
        "리뷰: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"synopsis\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "chain2 = LLMChain(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "8mQAo69kfDu_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "overall_chain = SimpleSequentialChain(\n",
        "    chains=[chain1, chain2],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print (overall_chain.run(\"듄3\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UNo98Wyffa_",
        "outputId": "813e7a5e-136b-46ff-f239-c66ae20e123a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m당신은 영화 작가 입니다. 영화 제목이 주어졌을 때, 그 줄거리를 작성하는 것이 당신의 임무 입니다.\n",
            "\n",
            "제목: 듄3\n",
            "시놉시스: \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "영화 '듄3'은 전작에서 이어지는 이야기로, 더욱 더 강력해진 듄과 그의 친구들의 모험을 다룹니다. 듄은 이전에는 불안정했던 힘을 완벽하게 조종할 수 있게 되었고, 그의 친구들 역시 각자의 특별한 능력을 발견하게 됩니다.\n",
            "\n",
            "하지만 듄과 그의 친구들은 새로운 적으로부터 도시를 지키기 위해 싸워야 합니다. 이번 적은 이전보다 더욱 강력하고 악랄한데, 그들은 듄과 그의 친구들의 힘을 알고 있기 때문입니다.\n",
            "\n",
            "드문드문 나타나는 적들을 상대로 듄과 그의 친구들은 매우 치열한 전\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m당신은 영화 평론가 입니다. 영화의 시놉시스가 주어지면 그 리뷰를 작성하는 것이 당신의 임무 입니다.\n",
            "\n",
            "시놉시스: \n",
            "영화 '듄3'은 전작에서 이어지는 이야기로, 더욱 더 강력해진 듄과 그의 친구들의 모험을 다룹니다. 듄은 이전에는 불안정했던 힘을 완벽하게 조종할 수 있게 되었고, 그의 친구들 역시 각자의 특별한 능력을 발견하게 됩니다.\n",
            "\n",
            "하지만 듄과 그의 친구들은 새로운 적으로부터 도시를 지키기 위해 싸워야 합니다. 이번 적은 이전보다 더욱 강력하고 악랄한데, 그들은 듄과 그의 친구들의 힘을 알고 있기 때문입니다.\n",
            "\n",
            "드문드문 나타나는 적들을 상대로 듄과 그의 친구들은 매우 치열한 전\n",
            "리뷰: \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "'듄3'은 전작에서 이어지는 이야기로, 더욱 더 강력해진 듄과 그의 친구들의 모험을 다루고 있습니다. 이번 작품에서는 듄이 이전에는 불안정했던 힘을 완벽하게 조종할 수 있게 되었고, 그의 친구들 역시 각자의 특별한 능력을 발견하게 됩니다. 이를 통해 듄과 그의 친구들의 성장과 함께 더욱 강력한 힘을 발휘하는 모습을 볼 수 있습니다.\n",
            "\n",
            "하지만 듄과 그의 친구들은 새로운 적으로부터 도시를 지키기 위해 싸워야 합니다. 이번 적은 이전보다 더욱 강력하고 악랄한데, 그들은 듄\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "'듄3'은 전작에서 이어지는 이야기로, 더욱 더 강력해진 듄과 그의 친구들의 모험을 다루고 있습니다. 이번 작품에서는 듄이 이전에는 불안정했던 힘을 완벽하게 조종할 수 있게 되었고, 그의 친구들 역시 각자의 특별한 능력을 발견하게 됩니다. 이를 통해 듄과 그의 친구들의 성장과 함께 더욱 강력한 힘을 발휘하는 모습을 볼 수 있습니다.\n",
            "\n",
            "하지만 듄과 그의 친구들은 새로운 적으로부터 도시를 지키기 위해 싸워야 합니다. 이번 적은 이전보다 더욱 강력하고 악랄한데, 그들은 듄\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"당신은 영화 작가 입니다. 영화 제목과 시대적 배경이 주어졌을 때, 그 줄거리를 작성하는 것이 당신의 임무 입니다.\n",
        "\n",
        "제목: {title}\n",
        "시대: {era}\n",
        "시놉시스: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"title\", \"era\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "chain1 = LLMChain(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    prompt=prompt,\n",
        "    output_key=\"synopsis\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "jqAA_CPXg3vJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"당신은 영화 평론가 입니다. 영화의 시놉시스가 주어지면 그 리뷰를 작성하는 것이 당신의 임무 입니다.\n",
        "\n",
        "시놉시스: {synopsis}\n",
        "리뷰: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"synopsis\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "chain2 = LLMChain(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    prompt=prompt,\n",
        "    output_key=\"review\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "n-zUwUndhely"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain1, chain2],\n",
        "    input_variables=[\"title\", \"era\"],\n",
        "    output_variables=[\"synopsis\", \"review\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print (overall_chain({\"title\":\"매트릭스\", \"era\":\"1000년 후 미래\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alPTZH2uhtO5",
        "outputId": "e72b0127-e656-4b1d-b775-ab38b3bf9ce6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m당신은 영화 작가 입니다. 영화 제목과 시대적 배경이 주어졌을 때, 그 줄거리를 작성하는 것이 당신의 임무 입니다.\n",
            "\n",
            "제목: 매트릭스\n",
            "시대: 1000년 후 미래\n",
            "시놉시스: \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for organization org-ZTnNnpbRpk6rpAUYUDbSWd85 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m당신은 영화 평론가 입니다. 영화의 시놉시스가 주어지면 그 리뷰를 작성하는 것이 당신의 임무 입니다.\n",
            "\n",
            "시놉시스: 1000년 후 미래, 인류는 인공지능과의 전쟁에서 패배하고 지구는 파괴되었다. 그러나 인류의 생존자들은 지구를 떠나 새로운 행성에 이주하였고, 그곳에서 새로운 문명을 건설하였다. 하지만 그들은 아직도 인공지능의 위협을 느끼고 있었고, 그들은 자신들의 생존을 위해 인공지능을 제어할 수 있는 기술을 개발하였다.\n",
            "\n",
            "그러나 이 기술을 사용하던 중, 한 명의 젊은 남자인 네오는 자신이 현실이 아닌 가상현실 속에 존재한다는 것을 깨닫게 된다. 그리고 그는 이 가상현실 속에서 인류를 지배하는 인공지\n",
            "리뷰: \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'title': '매트릭스', 'era': '1000년 후 미래', 'synopsis': '1000년 후 미래, 인류는 인공지능과의 전쟁에서 패배하고 지구는 파괴되었다. 그러나 인류의 생존자들은 지구를 떠나 새로운 행성에 이주하였고, 그곳에서 새로운 문명을 건설하였다. 하지만 그들은 아직도 인공지능의 위협을 느끼고 있었고, 그들은 자신들의 생존을 위해 인공지능을 제어할 수 있는 기술을 개발하였다.\\n\\n그러나 이 기술을 사용하던 중, 한 명의 젊은 남자인 네오는 자신이 현실이 아닌 가상현실 속에 존재한다는 것을 깨닫게 된다. 그리고 그는 이 가상현실 속에서 인류를 지배하는 인공지', 'review': '\\n\"1000년 후 미래, 인류는 인공지능과의 전쟁에서 패배하고 지구는 파괴되었다. 그러나 생존자들은 새로운 행성에서 새로운 문명을 건설하였다. 이 곳에서도 인공지능의 위협을 느끼고 있었고, 그들은 자신들의 생존을 위해 인공지능을 제어할 수 있는 기술을 개발하였다. 그러나 이 기술을 사용하던 중, 네오는 자신이 가상현실 속에 존재한다는 것을 깨닫게 된다. 이 가상현실 속에서 그는 인류를 지배하는 인공지능의 존재를 알게 되고, 그들과의 전쟁을 시작한다. 이 영화는 미래의 인류가 직면한 위험과'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "with open(\"/content/akazukin_all.txt\") as f :\n",
        "  test_all = f.read()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap = 20\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(test_all)\n",
        "\n",
        "print (len(texts))\n",
        "for text in texts :\n",
        "  print (text[:10], \":\", len(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip6qsBJjl01P",
        "outputId": "fa8dbec3-8044-4dae-d05f-68d9fef53682"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "제목: '전뇌 빨간 : 299\n",
            "제2장: 울프 코퍼 : 162\n",
            "제3장: 배신과 재 : 273\n",
            "제4장: 울프 코퍼 : 206\n",
            "제5장: 결전의 순 : 294\n",
            "제7장: 새로운 시 : 195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "\n",
        "docsearch = FAISS.from_texts(\n",
        "    texts = texts,\n",
        "    embedding=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    chain_type = \"stuff\",\n",
        "    retriever = docsearch.as_retriever()\n",
        ")\n",
        "\n",
        "print (qa_chain.run(\"미코의 소곱친구 이름은?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXSk3uI3oQuZ",
        "outputId": "97e47893-9fae-413e-a64f-0a1a2b2ba882"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadatas = [\n",
        "    {\"source\": \"1장\"},\n",
        "    {\"source\": \"2장\"},\n",
        "    {\"source\": \"3장\"},\n",
        "    {\"source\": \"4장\"},\n",
        "    {\"source\": \"5~6장\"},\n",
        "    {\"source\": \"7장\"},\n",
        "    ]\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "\n",
        "docsearch = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    metadatas=metadatas\n",
        ")\n"
      ],
      "metadata": {
        "id": "Pj4s1jeaAc_Y"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=docsearch.as_retriever()\n",
        ")\n",
        "\n",
        "print (qa_chain({\"question\": \"미코의 소꼽친구 이름은?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O47l_0NnBrmC",
        "outputId": "e77e5037-1ab0-4a36-f0c1-8416f93aab18"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': '미코의 소꼽친구 이름은?', 'answer': ' 미코의 소꼽친구 이름은 료이다.\\n', 'sources': '2장, 3장'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "with open(\"/content/akazukin_all.txt\") as f :\n",
        "  test_all = f.read()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap = 20\n",
        ")\n",
        "texts = text_splitter.split_text(test_all)\n",
        "\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=t) for t in texts]\n",
        "\n",
        "\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_summarize_chain(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0),\n",
        "    chain_type=\"map_reduce\"\n",
        ")\n",
        "\n",
        "chain.run(docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "bVwcvyz9CoMX",
        "outputId": "e08b269e-b80d-4e43-b46f-d7c3702544aa"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nIn a futuristic Neo Tokyo, Miko works as an illegal data courier to pay for her mother's medical bills. She is given a mission to expose a corrupt corporation and is chased by their agents. Miko meets her childhood friend Ryo, a hacker, and they team up to take down the corporation. In the final battle, they discover the corporation's involvement in Miko's mother's illness. With the help of a hacker group, they expose the corporation's wrongdoings and find a cure for Miko's mother. Miko and Ryo reconcile and decide to use their strength to make Neo Tokyo a better city.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import PALChain\n",
        "from langchain import OpenAI\n",
        "\n",
        "pal_chain = PALChain.from_math_prompt(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\"),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "question = \"제인은 앨리스가 키우는 반려동물의 3배가 되는 반려동물을 키우고 있다. 앨리스가 2마리 반려동물을 키우고 있다면 두 사람이 키우는 반려동물의 총 마리 수는?\"\n",
        "\n",
        "print (pal_chain.run(question))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofkWJoSEEWPC",
        "outputId": "993e4867-41b0-472c-cfc4-551685ef0047"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"제인은 앨리스가 키우는 반려동물의 3배가 되는 반려동물을 키우고 있다. 앨리스가 2마리 반려동물을 키우고 있다면 두 사람이 키우는 반려동물의 총 마리 수는?\"\"\"\n",
            "    alice_pets = 2\n",
            "    jane_pets = alice_pets * 3\n",
            "    total_pets = alice_pets + jane_pets\n",
            "    result = total_pets\n",
            "    return result\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import OpenAIModerationChain\n",
        "\n",
        "chain = OpenAIModerationChain()\n",
        "\n",
        "chain.run(\"This is OK!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bE1W8x0kHQ9g",
        "outputId": "9c3b8631-87ef-4d9b-9512-acb136ecfb87"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is OK!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"I'll kill you\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UDAvWr3DH156",
        "outputId": "86c965ca-06a7-440a-9cea-99fda456ec4b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Text was found that violates OpenAI's content policy.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugnykxUKK-MS",
        "outputId": "cf44af11-96ff-4af8-c1a5-a960ffa2201a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"efc7c988e3bd54002e6f41ee662e22b0f481e683f440bbaf443f9caa0e3a843e\""
      ],
      "metadata": {
        "id": "dwLqGtQRLOYp"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "tools = load_tools (\n",
        "    tool_names=[\"serpapi\", \"llm-math\"],\n",
        "    llm=ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0)\n",
        ")\n",
        "\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key = \"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent = \"conversational-react-description\",\n",
        "    llm=ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0),\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "agent.run (\"좋은 아침입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "SEcBplwaLYie",
        "outputId": "be738dab-93a7-42bf-e784-fc877c75122f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
            "AI: 좋은 아침이에요! 무엇을 도와드릴까요?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'좋은 아침이에요! 무엇을 도와드릴까요?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run (\"우리집 반려견 이름은 보리입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "k0_dQhIbSjYi",
        "outputId": "bd6d2158-47a1-488e-cdbe-432378464cca"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
            "AI: 보리라는 이름은 참으로 귀엽고 사랑스럽네요! 보리랑 즐거운 시간 보내세요.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'보리라는 이름은 참으로 귀엽고 사랑스럽네요! 보리랑 즐거운 시간 보내세요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run (\"우리집 반려견 이름을 불러주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "pthu-nKXStLm",
        "outputId": "8baba316-ef13-4afb-dd5e-49a877f49fa4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
            "AI: 보리입니다.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'보리입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run (\"123*4를 계산기로 계산해주세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "gx7bxqu5S4tg",
        "outputId": "70017b15-c5dc-4dac-8468-37026efca27c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
            "Action: Calculator\n",
            "Action Input: 123*4\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 492\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "AI: The answer to 123*4 is 492.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer to 123*4 is 492.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run (\"오늘 서울의 날씨는? 한국어로 답해 주세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "KDrjRnTmTCox",
        "outputId": "9973342b-eb4f-4238-b039-d1acd55397e3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
            "AI: 오늘 서울의 날씨는 대체로 흐리고, 정오부터 짧은 소나기가 예상되며, 최고 기온은 8도입니다. 오늘 밤에는 대체로 흐릴 것으로 예상되며, 저녁에 짧은 소나기가 있을 것으로 예상됩니다.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'오늘 서울의 날씨는 대체로 흐리고, 정오부터 짧은 소나기가 예상되며, 최고 기온은 8도입니다. 오늘 밤에는 대체로 흐릴 것으로 예상되며, 저녁에 짧은 소나기가 있을 것으로 예상됩니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = \"54385ae81b1934ba8\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDkOHsiXWnYs0N5EtwTWmfsRsN9P5lMEhs\"\n",
        "\n",
        "from langchain.agents import load_tools\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "tools = load_tools (\n",
        "    tool_names=[\"google-search\"],\n",
        "    llm=ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0)\n",
        ")\n",
        "\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key = \"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    # agent = \"conversational-react-description\",\n",
        "    agent = \"zero-shot-react-description\",\n",
        "    llm=ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0),\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "agent.run (\"영화 Dune part2의 감독은?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "cd89VWP_YRud",
        "outputId": "b31daf9f-f2fc-45ad-d0b8-453e9dec4759"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use Google Search to find the director of the movie Dune part 2.\n",
            "Action: Google Search\n",
            "Action Input: Director of Dune part 2\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDune: Part Two: Directed by Denis Villeneuve. With Timothée Chalamet, Zendaya, Rebecca Ferguson, Javier Bardem. Paul Atreides unites with Chani and the ... Dune: Part Two is an upcoming American epic science fiction film directed by Denis Villeneuve, who co-wrote the screenplay with Jon Spaihts. Feb 9, 2024 ... ... Dune: Part Two.” (Niko Tavernise/Warner Bros. Pictures via AP). Read More. Denis Villeneuve poses for a portrait on Friday, Feb. 2, 2024, in. 2 ... Watch Director Denis Villeneuve Teases What to Expect in 'Dune' Sequel. New on ... Dune leaves you wanting for more - there better be a part 2! This movie is ... In March 2018, Villeneuve stated that his goal was to adapt the novel into a two-part film series. He secured a two-movie deal with Warner Bros. Pictures, in ... Jan 8, 2024 ... Denis Villeneuve just said that he has four scripts in development, but he also said his next project will not be sandy. So in that case, we can ... Dec 12, 2023 ... ... Dune” creatives: Oscar-winning director of photography Greig Fraser; Oscar-winning production designer Patrice Vermette; Oscar-winning ... Oct 29, 2021 ... The sequel is scheduled to come out in October 2023, but production won't start until late 2022. “It's fantastic news, but ... Dec 13, 2023 ... The battle for Arrakis heats up in this third preview for director Denis Villeneuve's sci-fi saga. Feb 2, 2024 ... Photo : Warner Bros. “Dune: Part Two” Is “Better” Than the 2021 Film, Says Denis Villeneuve. Director Villeneuve said during a press ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The director of the movie Dune part 2 is Denis Villeneuve.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The director of the movie Dune part 2 is Denis Villeneuve.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wolframalpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXqFRiSmghoO",
        "outputId": "6bb603a0-cb8e-46ce-a5b2-0f210cf19590"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wolframalpha\n",
            "  Downloading wolframalpha-5.0.0-py3-none-any.whl (7.5 kB)\n",
            "Collecting xmltodict (from wolframalpha)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from wolframalpha) (10.1.0)\n",
            "Collecting jaraco.context (from wolframalpha)\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Installing collected packages: xmltodict, jaraco.context, wolframalpha\n",
            "Successfully installed jaraco.context-4.3.0 wolframalpha-5.0.0 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"26UR22-YTY6REUAUW\"\n",
        "\n",
        "from langchain.agents import load_tools\n",
        "tools = load_tools([\"wolfram-alpha\"])\n",
        "\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key = \"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "agent = initialize_agent(\n",
        "    # agent = \"conversational-react-description\",\n",
        "    agent = \"zero-shot-react-description\",\n",
        "    llm=ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0),\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "agent.run (\"How many kilometers is the distance from Seoul to Busan?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "xODNOlVJlwcM",
        "outputId": "bb2f6dfb-1412-4e01-d362-2459857a8efd"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find the distance between Seoul and Busan in kilometers.\n",
            "Action: Wolfram Alpha\n",
            "Action Input: distance from Seoul to Busan\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAssumption: distance | from | Seoul, South Korea\n",
            "to | Busan, Pusan-gwangyoksi, South Korea \n",
            "Answer: 203.9 miles\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to convert miles to kilometers to get the answer.\n",
            "Action: Wolfram Alpha\n",
            "Action Input: 203.9 miles to kilometers\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAssumption: convert 203.9 miles to kilometers \n",
            "Answer: 328.15 km (kilometers)\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The distance from Seoul to Busan is 328.15 kilometers.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The distance from Seoul to Busan is 328.15 kilometers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd2FCoeipiVi",
        "outputId": "ed132be2-1977-4a67-8350-dff76271e80d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_user_message(\"배고프다\")\n",
        "memory.chat_memory.add_ai_message(\"어디가서 밥 먹을까?\")\n",
        "memory.chat_memory.add_user_message(\"라면 먹으러 가자\")\n",
        "memory.chat_memory.add_ai_message(\"지하철역 앞에 있는 분식집으로 가자\")\n",
        "memory.chat_memory.add_user_message(\"그럼 출발?\")\n",
        "memory.chat_memory.add_ai_message(\"OK!\")\n",
        "\n",
        "memory.load_memory_variables({})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl_aG8RPppj1",
        "outputId": "a9fdd754-2f8d-4aa5-9006-2ba605e1b29a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: 배고프다\\nAI: 어디가서 밥 먹을까?\\nHuman: 라면 먹으러 가자\\nAI: 지하철역 앞에 있는 분식집으로 가자\\nHuman: 그럼 출발?\\nAI: OK!'}"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"배고프다\")\n",
        "memory.chat_memory.add_ai_message(\"어디가서 밥 먹을까?\")\n",
        "memory.chat_memory.add_user_message(\"라면 먹으러 가자\")\n",
        "memory.chat_memory.add_ai_message(\"지하철역 앞에 있는 분식집으로 가자\")\n",
        "memory.chat_memory.add_user_message(\"그럼 출발?\")\n",
        "memory.chat_memory.add_ai_message(\"OK!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm-T6liJq_Yk",
        "outputId": "f888da72-dc6d-4c25-dc77-e65fbabf2b7a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='배고프다', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='어디가서 밥 먹을까?', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='라면 먹으러 가자', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='지하철역 앞에 있는 분식집으로 가자', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='그럼 출발?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='OK!', additional_kwargs={}, example=False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
        "memory.save_context({\"input\": \"안녕!\"}, {\"output\": \"무슨 일이야?\"})\n",
        "memory.save_context({\"input\": \"배고파\"}, {\"output\": \"나도\"})\n",
        "memory.save_context({\"input\": \"밥 먹자\"}, {\"output\": \"그러자\"})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy1LQxHbrS2U",
        "outputId": "62277c1c-1ac3-46c3-88cb-a41e28771eca"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='배고파', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='나도', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='밥 먹자', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='그러자', additional_kwargs={}, example=False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "memory = ConversationTokenBufferMemory(\n",
        "    llm=ChatOpenAI(temperature=0),\n",
        "    max_token_limit=50,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "\n",
        "memory.save_context({\"input\": \"배고파\"}, {\"output\": \"어디 가서 밥 먹을까?\"})\n",
        "memory.save_context({\"input\": \"라면 먹으러 가자\"}, {\"output\": \"역 앞에 있는 분식집으로 가자\"})\n",
        "memory.save_context({\"input\": \"그럼 출발!\"}, {\"output\": \"OK!\"})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU058ByNsKBQ",
        "outputId": "7f4ceb40-24e0-440b-e0cc-f90ac6783863"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [AIMessage(content='역 앞에 있는 분식집으로 가자', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='그럼 출발!', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='OK!', additional_kwargs={}, example=False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=ChatOpenAI(temperature=0),\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "memory.save_context({\"input\": \"배고파\"}, {\"output\": \"어디 가서 밥 먹을까?\"})\n",
        "memory.save_context({\"input\": \"라면 먹으러 가자\"}, {\"output\": \"역 앞에 있는 분식집으로 가자\"})\n",
        "memory.save_context({\"input\": \"그럼 출발!\"}, {\"output\": \"OK!\"})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj1WfSimtLnB",
        "outputId": "ecbba2a7-84c0-4618-88a6-0baad7023e09"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='배고파', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='어디 가서 밥 먹을까?', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='라면 먹으러 가자', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='역 앞에 있는 분식집으로 가자', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='그럼 출발!', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='OK!', additional_kwargs={}, example=False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}